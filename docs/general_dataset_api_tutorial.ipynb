{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POP API Tutorial: General Dataset Management\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sci-ndp/pop/blob/main/docs/general_dataset_api_tutorial.ipynb)\n",
        "[![Open in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sci-ndp/pop/main?filepath=docs/general_dataset_api_tutorial.ipynb)\n",
        "\n",
        "> 🚀 **Run Online Options:**\n",
        "> - **Google Colab**: Dependencies installed automatically in the first cell\n",
        "> - **Binder**: Pre-configured environment, ready to run immediately\n",
        "> - **Local**: Requires `pip install requests jupyter`\n",
        "\n",
        "This notebook demonstrates how to use the POP (Point of Presence) API to manage datasets programmatically. You will learn how to:\n",
        "\n",
        "1. **Authenticate** with the API using Keycloak tokens\n",
        "2. **Create organizations** to organize your datasets\n",
        "3. **Create comprehensive datasets** with metadata, tags, groups, and resources\n",
        "4. **Update and manage** existing datasets\n",
        "5. **Handle errors** and best practices\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.7+\n",
        "- `requests` library\n",
        "- Access to a POP API instance\n",
        "- Valid authentication credentials\n",
        "\n",
        "## API Overview\n",
        "\n",
        "The POP API provides both specialized endpoints (for S3, Kafka, URLs) and general dataset endpoints for flexible data management. This tutorial focuses on the general dataset endpoints which offer maximum flexibility for diverse data types and structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install requests -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "First, let's import the necessary libraries and configure our API connection parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from typing import Dict, Any, Optional\n",
        "import time\n",
        "\n",
        "# Pretty printing for JSON responses\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Variables\n",
        "\n",
        "**Important:** Replace these values with your actual API endpoint and credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Base URL: http://localhost:8001\n",
            "CKAN Server: local\n",
            "Token configured: ✗ Please set your token\n"
          ]
        }
      ],
      "source": [
        "# API Configuration\n",
        "API_BASE_URL = \"http://localhost:8001\"  # Replace with your API URL\n",
        "CKAN_SERVER = \"local\"  # Options: \"local\" or \"pre_ckan\"\n",
        "\n",
        "# Authentication Token\n",
        "# You can obtain this token from your Keycloak instance or use the /token endpoint\n",
        "AUTH_TOKEN = \"your-keycloak-token-here\"  # Replace with your actual token\n",
        "\n",
        "# Request headers with authentication\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "print(f\"API Base URL: {API_BASE_URL}\")\n",
        "print(f\"CKAN Server: {CKAN_SERVER}\")\n",
        "print(f\"Token configured: {'✓' if AUTH_TOKEN != 'your-keycloak-token-here' else '✗ Please set your token'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions\n",
        "\n",
        "Let's create some utility functions to make our API interactions cleaner and more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_api_request(method: str, endpoint: str, data: Optional[Dict] = None, \n",
        "                    params: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Make an API request with proper error handling.\n",
        "    \n",
        "    Args:\n",
        "        method: HTTP method (GET, POST, PUT, PATCH, DELETE)\n",
        "        endpoint: API endpoint (e.g., '/dataset', '/organization')\n",
        "        data: Request payload for POST/PUT/PATCH requests\n",
        "        params: Query parameters\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing the response data\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE_URL}{endpoint}\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.request(\n",
        "            method=method,\n",
        "            url=url,\n",
        "            headers=HEADERS,\n",
        "            json=data,\n",
        "            params=params\n",
        "        )\n",
        "        \n",
        "        print(f\"🔗 {method} {url}\")\n",
        "        print(f\"📊 Status: {response.status_code}\")\n",
        "        \n",
        "        if response.status_code in [200, 201]:\n",
        "            result = response.json()\n",
        "            print(\"✅ Success!\")\n",
        "            return result\n",
        "        else:\n",
        "            print(f\"❌ Error: {response.status_code}\")\n",
        "            try:\n",
        "                error_detail = response.json()\n",
        "                print(f\"Error details: {json.dumps(error_detail, indent=2)}\")\n",
        "            except:\n",
        "                print(f\"Error text: {response.text}\")\n",
        "            return {\"error\": True, \"status_code\": response.status_code, \"detail\": response.text}\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request failed: {e}\")\n",
        "        return {\"error\": True, \"exception\": str(e)}\n",
        "\n",
        "def print_response(response: Dict[str, Any], title: str = \"Response\"):\n",
        "    \"\"\"\n",
        "    Pretty print API responses.\n",
        "    \"\"\"\n",
        "    print(f\"\\n📋 {title}:\")\n",
        "    print(\"─\" * 50)\n",
        "    pprint(response)\n",
        "    print(\"─\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication Test\n",
        "\n",
        "Let's verify that our authentication is working by checking the API status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /status (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57DEC1A0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 API Status:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /status (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57DEC1A0>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "⚠️  API connectivity issues. Please check your configuration.\n"
          ]
        }
      ],
      "source": [
        "# Test API connectivity and authentication\n",
        "status_response = make_api_request(\"GET\", \"/status\")\n",
        "print_response(status_response, \"API Status\")\n",
        "\n",
        "if \"error\" not in status_response:\n",
        "    print(\"🎉 API is accessible and responding correctly!\")\n",
        "else:\n",
        "    print(\"⚠️  API connectivity issues. Please check your configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Organization Management\n",
        "\n",
        "Before creating datasets, we need to ensure we have an organization to contain them. Organizations in CKAN serve as containers and provide access control for datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Existing Organizations\n",
        "\n",
        "First, let's see what organizations already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /organization?server=local (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57CF65D0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 Existing Organizations:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /organization?server=local (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57CF65D0>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "⚠️  Unable to retrieve organizations list\n"
          ]
        }
      ],
      "source": [
        "# List existing organizations\n",
        "params = {\"server\": CKAN_SERVER}\n",
        "organizations = make_api_request(\"GET\", \"/organization\", params=params)\n",
        "print_response(organizations, \"Existing Organizations\")\n",
        "\n",
        "if isinstance(organizations, list):\n",
        "    print(f\"\\n📈 Found {len(organizations)} organizations\")\n",
        "    if organizations:\n",
        "        print(\"Organizations:\")\n",
        "        for i, org in enumerate(organizations[:5], 1):  # Show first 5\n",
        "            print(f\"  {i}. {org}\")\n",
        "        if len(organizations) > 5:\n",
        "            print(f\"  ... and {len(organizations) - 5} more\")\n",
        "else:\n",
        "    print(\"⚠️  Unable to retrieve organizations list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a New Organization\n",
        "\n",
        "Now let's create a new organization for our tutorial. We'll use a timestamped name to avoid conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏢 Creating organization with data:\n",
            "\n",
            "📋 Organization Data:\n",
            "──────────────────────────────────────────────────\n",
            "{'description': 'An organization created for the POP API tutorial '\n",
            "                'demonstration. This organization showcases best practices for '\n",
            "                'data management and dataset organization.',\n",
            " 'name': 'research_tutorial_20250628_234442',\n",
            " 'title': 'Research Tutorial Organization - 20250628_234442'}\n",
            "──────────────────────────────────────────────────\n",
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /organization?server=local (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57CF65D0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 Organization Creation Response:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /organization?server=local (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57CF65D0>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "❌ Failed to create organization\n"
          ]
        }
      ],
      "source": [
        "# Organization data\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "organization_data = {\n",
        "    \"name\": f\"research_tutorial_{timestamp}\",\n",
        "    \"title\": f\"Research Tutorial Organization - {timestamp}\",\n",
        "    \"description\": \"An organization created for the POP API tutorial demonstration. This organization showcases best practices for data management and dataset organization.\"\n",
        "}\n",
        "\n",
        "print(\"🏢 Creating organization with data:\")\n",
        "print_response(organization_data, \"Organization Data\")\n",
        "\n",
        "# Create the organization\n",
        "params = {\"server\": CKAN_SERVER}\n",
        "org_response = make_api_request(\"POST\", \"/organization\", data=organization_data, params=params)\n",
        "print_response(org_response, \"Organization Creation Response\")\n",
        "\n",
        "if \"error\" not in org_response and \"id\" in org_response:\n",
        "    organization_id = organization_data[\"name\"]  # Use name as ID for dataset creation\n",
        "    print(f\"\\n✅ Organization created successfully!\")\n",
        "    print(f\"🆔 Organization ID: {organization_id}\")\n",
        "    print(f\"📋 Organization Name: {organization_data['name']}\")\n",
        "else:\n",
        "    print(\"❌ Failed to create organization\")\n",
        "    organization_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Creation\n",
        "\n",
        "Now we'll create a comprehensive dataset using the general dataset endpoint. This dataset will demonstrate all the available features including metadata, tags, groups, and multiple resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Dataset Payload\n",
        "\n",
        "Let's prepare a comprehensive dataset that showcases the full capabilities of the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Prepared comprehensive dataset payload:\n",
            "\n",
            "📋 Dataset Payload:\n",
            "──────────────────────────────────────────────────\n",
            "{'extras': {'access_restrictions': 'None - Open Access',\n",
            "            'backup_location': 'Cloud Storage Tier 1',\n",
            "            'contact_email': 'climate-data@university.edu',\n",
            "            'data_collection_period': '2023-01-01 to 2024-12-31',\n",
            "            'data_format_standard': 'CF-1.8',\n",
            "            'doi': '10.5194/essd-2024-example',\n",
            "            'funding_agency': 'National Science Foundation',\n",
            "            'geographical_coverage': 'North America',\n",
            "            'methodology': 'Automated weather station network',\n",
            "            'principal_investigator': 'Dr. Jane Smith',\n",
            "            'project_code': 'CLIMATE2024',\n",
            "            'publication_status': 'Published',\n",
            "            'quality_level': 'Level 2 - Processed',\n",
            "            'temporal_resolution': 'Daily',\n",
            "            'update_frequency': 'Monthly'},\n",
            " 'groups': ['science', 'climate', 'research'],\n",
            " 'license_id': 'cc-by-4.0',\n",
            " 'name': 'climate_research_dataset_20250628_234442',\n",
            " 'notes': 'This dataset contains comprehensive climate research data including '\n",
            "          'temperature, precipitation, and atmospheric measurements. Created '\n",
            "          'as part of the POP API tutorial to demonstrate best practices for '\n",
            "          'dataset management and metadata organization.',\n",
            " 'owner_org': 'research_tutorial_20250628_234442',\n",
            " 'private': False,\n",
            " 'resources': [{'description': 'Daily temperature readings from 500+ weather '\n",
            "                               'stations across North America. Includes min, '\n",
            "                               'max, and average temperatures with quality '\n",
            "                               'flags.',\n",
            "                'format': 'CSV',\n",
            "                'mimetype': 'text/csv',\n",
            "                'name': 'Daily Temperature Measurements',\n",
            "                'size': 15728640,\n",
            "                'url': 'https://data.climate-research.org/temperature/daily_temp_2024.csv'},\n",
            "               {'description': 'Monthly precipitation totals with statistical '\n",
            "                               'summaries and anomaly calculations relative to '\n",
            "                               '30-year climate normals.',\n",
            "                'format': 'JSON',\n",
            "                'mimetype': 'application/json',\n",
            "                'name': 'Monthly Precipitation Data',\n",
            "                'size': 2097152,\n",
            "                'url': 'https://data.climate-research.org/precipitation/monthly_precip_2024.json'},\n",
            "               {'description': 'Comprehensive atmospheric data including '\n",
            "                               'barometric pressure, humidity, wind speed and '\n",
            "                               'direction, formatted according to '\n",
            "                               'meteorological XML standards.',\n",
            "                'format': 'XML',\n",
            "                'mimetype': 'application/xml',\n",
            "                'name': 'Atmospheric Measurements',\n",
            "                'size': 8388608,\n",
            "                'url': 'https://data.climate-research.org/atmospheric/pressure_humidity.xml'},\n",
            "               {'description': 'High-resolution GeoTIFF raster showing climate '\n",
            "                               'zone classifications based on temperature and '\n",
            "                               'precipitation patterns. Includes metadata for '\n",
            "                               'coordinate reference system.',\n",
            "                'format': 'GeoTIFF',\n",
            "                'mimetype': 'image/tiff',\n",
            "                'name': 'Climate Zone Classification',\n",
            "                'size': 52428800,\n",
            "                'url': 'https://data.climate-research.org/spatial/climate_zones.tif'}],\n",
            " 'tags': ['climate',\n",
            "          'environment',\n",
            "          'timeseries',\n",
            "          'weather',\n",
            "          'research',\n",
            "          'tutorial'],\n",
            " 'title': 'Comprehensive Climate Research Dataset - Tutorial Example',\n",
            " 'version': '1.0'}\n",
            "──────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive dataset payload\n",
        "dataset_payload = {\n",
        "    # Required fields\n",
        "    \"name\": f\"climate_research_dataset_{timestamp}\",\n",
        "    \"title\": \"Comprehensive Climate Research Dataset - Tutorial Example\",\n",
        "    \"owner_org\": organization_data['name'],  # Use our created organization\n",
        "    \n",
        "    # Descriptive metadata\n",
        "    \"notes\": \"This dataset contains comprehensive climate research data including temperature, precipitation, and atmospheric measurements. Created as part of the POP API tutorial to demonstrate best practices for dataset management and metadata organization.\",\n",
        "    \n",
        "    # Categorization\n",
        "    \"tags\": [\"climate\", \"environment\", \"timeseries\", \"weather\", \"research\", \"tutorial\"],\n",
        "    \"groups\": [\"science\", \"climate\", \"research\"],\n",
        "    \n",
        "    # Administrative metadata\n",
        "    \"license_id\": \"cc-by-4.0\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"private\": False,\n",
        "    \n",
        "    # Custom metadata using extras\n",
        "    \"extras\": {\n",
        "        \"project_code\": \"CLIMATE2024\",\n",
        "        \"funding_agency\": \"National Science Foundation\",\n",
        "        \"principal_investigator\": \"Dr. Jane Smith\",\n",
        "        \"data_collection_period\": \"2023-01-01 to 2024-12-31\",\n",
        "        \"geographical_coverage\": \"North America\",\n",
        "        \"temporal_resolution\": \"Daily\",\n",
        "        \"quality_level\": \"Level 2 - Processed\",\n",
        "        \"contact_email\": \"climate-data@university.edu\",\n",
        "        \"methodology\": \"Automated weather station network\",\n",
        "        \"data_format_standard\": \"CF-1.8\",\n",
        "        \"doi\": \"10.5194/essd-2024-example\",\n",
        "        \"publication_status\": \"Published\",\n",
        "        \"access_restrictions\": \"None - Open Access\",\n",
        "        \"update_frequency\": \"Monthly\",\n",
        "        \"backup_location\": \"Cloud Storage Tier 1\"\n",
        "    },\n",
        "    \n",
        "    # Associated resources\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/temperature/daily_temp_2024.csv\",\n",
        "            \"format\": \"CSV\",\n",
        "            \"name\": \"Daily Temperature Measurements\",\n",
        "            \"description\": \"Daily temperature readings from 500+ weather stations across North America. Includes min, max, and average temperatures with quality flags.\",\n",
        "            \"mimetype\": \"text/csv\",\n",
        "            \"size\": 15728640  # ~15MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/precipitation/monthly_precip_2024.json\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"name\": \"Monthly Precipitation Data\",\n",
        "            \"description\": \"Monthly precipitation totals with statistical summaries and anomaly calculations relative to 30-year climate normals.\",\n",
        "            \"mimetype\": \"application/json\",\n",
        "            \"size\": 2097152  # ~2MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/atmospheric/pressure_humidity.xml\",\n",
        "            \"format\": \"XML\",\n",
        "            \"name\": \"Atmospheric Measurements\",\n",
        "            \"description\": \"Comprehensive atmospheric data including barometric pressure, humidity, wind speed and direction, formatted according to meteorological XML standards.\",\n",
        "            \"mimetype\": \"application/xml\",\n",
        "            \"size\": 8388608  # ~8MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/spatial/climate_zones.tif\",\n",
        "            \"format\": \"GeoTIFF\",\n",
        "            \"name\": \"Climate Zone Classification\",\n",
        "            \"description\": \"High-resolution GeoTIFF raster showing climate zone classifications based on temperature and precipitation patterns. Includes metadata for coordinate reference system.\",\n",
        "            \"mimetype\": \"image/tiff\",\n",
        "            \"size\": 52428800  # ~50MB\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"📊 Prepared comprehensive dataset payload:\")\n",
        "print_response(dataset_payload, \"Dataset Payload\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Dataset\n",
        "\n",
        "Now let's create the dataset using our comprehensive payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  Cannot create dataset without a valid organization\n"
          ]
        }
      ],
      "source": [
        "if organization_id:\n",
        "    print(\"🚀 Creating comprehensive dataset...\")\n",
        "    \n",
        "    # Create the dataset\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    dataset_response = make_api_request(\"POST\", \"/dataset\", data=dataset_payload, params=params)\n",
        "    print_response(dataset_response, \"Dataset Creation Response\")\n",
        "    \n",
        "    if \"error\" not in dataset_response and \"id\" in dataset_response:\n",
        "        dataset_id = dataset_response[\"id\"]\n",
        "        print(f\"\\n🎉 Dataset created successfully!\")\n",
        "        print(f\"🆔 Dataset ID: {dataset_id}\")\n",
        "        print(f\"📋 Dataset Name: {dataset_payload['name']}\")\n",
        "        print(f\"🏷️  Tags: {', '.join(dataset_payload['tags'])}\")\n",
        "        print(f\"👥 Groups: {', '.join(dataset_payload['groups'])}\")\n",
        "        print(f\"📁 Resources: {len(dataset_payload['resources'])} files\")\n",
        "        print(f\"🏢 Organization: {organization_id}\")\n",
        "    else:\n",
        "        print(\"❌ Failed to create dataset\")\n",
        "        dataset_id = None\n",
        "else:\n",
        "    print(\"⚠️  Cannot create dataset without a valid organization\")\n",
        "    dataset_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Updates\n",
        "\n",
        "The API supports both full updates (PUT) and partial updates (PATCH). Let's demonstrate both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partial Update (PATCH)\n",
        "\n",
        "PATCH updates allow you to modify specific fields without affecting others. This is ideal for incremental changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  No dataset available for update\n"
          ]
        }
      ],
      "source": [
        "if dataset_id:\n",
        "    print(\"🔄 Performing partial update (PATCH)...\")\n",
        "    \n",
        "    # Partial update payload - only fields we want to change\n",
        "    partial_update = {\n",
        "        \"version\": \"1.1\",\n",
        "        \"notes\": dataset_payload[\"notes\"] + \"\\n\\n**Update Log:**\\n- Version 1.1: Added additional quality control measures and enhanced metadata.\",\n",
        "        \"extras\": {\n",
        "            \"last_updated\": datetime.datetime.now().isoformat(),\n",
        "            \"update_type\": \"Quality enhancement\",\n",
        "            \"quality_level\": \"Level 3 - Validated\",\n",
        "            \"validation_status\": \"Completed\",\n",
        "            \"revision_notes\": \"Enhanced quality control procedures applied\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"📝 Partial update payload:\")\n",
        "    print_response(partial_update, \"Partial Update Data\")\n",
        "    \n",
        "    # Apply the partial update\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    patch_response = make_api_request(\"PATCH\", f\"/dataset/{dataset_id}\", data=partial_update, params=params)\n",
        "    print_response(patch_response, \"Partial Update Response\")\n",
        "    \n",
        "    if \"error\" not in patch_response:\n",
        "        print(\"\\n✅ Partial update completed successfully!\")\n",
        "        print(\"📈 Updated fields: version, notes, extras\")\n",
        "        print(\"🔒 Preserved fields: title, tags, groups, resources, etc.\")\n",
        "    else:\n",
        "        print(\"❌ Partial update failed\")\n",
        "else:\n",
        "    print(\"⚠️  No dataset available for update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Update (PUT)\n",
        "\n",
        "PUT updates replace the entire dataset. This is useful when you want to make comprehensive changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  No dataset available for update\n"
          ]
        }
      ],
      "source": [
        "if dataset_id:\n",
        "    print(\"🔄 Performing full update (PUT)...\")\n",
        "    \n",
        "    # Full update - modify the original payload\n",
        "    full_update = dataset_payload.copy()\n",
        "    full_update.update({\n",
        "        \"version\": \"2.0\",\n",
        "        \"title\": \"Comprehensive Climate Research Dataset - Updated Edition\",\n",
        "        \"tags\": dataset_payload[\"tags\"] + [\"updated\", \"validated\", \"v2\"],\n",
        "        \"license_id\": \"cc-by-sa-4.0\",  # Changed license\n",
        "    })\n",
        "    \n",
        "    # Update extras with new information\n",
        "    full_update[\"extras\"].update({\n",
        "        \"major_update\": datetime.datetime.now().isoformat(),\n",
        "        \"update_type\": \"Major revision\",\n",
        "        \"quality_level\": \"Level 4 - Research Grade\",\n",
        "        \"peer_review_status\": \"Completed\",\n",
        "        \"publication_doi\": \"10.5194/essd-2024-updated\",\n",
        "        \"citation_count\": \"0\",\n",
        "        \"download_statistics\": \"Available via API\"\n",
        "    })\n",
        "    \n",
        "    # Add a new resource\n",
        "    full_update[\"resources\"].append({\n",
        "        \"url\": \"https://data.climate-research.org/documentation/dataset_methodology_v2.pdf\",\n",
        "        \"format\": \"PDF\",\n",
        "        \"name\": \"Methodology Documentation v2.0\",\n",
        "        \"description\": \"Comprehensive methodology documentation including data collection procedures, quality control measures, and validation protocols for version 2.0.\",\n",
        "        \"mimetype\": \"application/pdf\",\n",
        "        \"size\": 1048576  # ~1MB\n",
        "    })\n",
        "    \n",
        "    print(\"📝 Full update includes:\")\n",
        "    print(f\"  • Updated title: {full_update['title']}\")\n",
        "    print(f\"  • New version: {full_update['version']}\")\n",
        "    print(f\"  • Updated license: {full_update['license_id']}\")\n",
        "    print(f\"  • Additional tags: {len(full_update['tags'])} total\")\n",
        "    print(f\"  • Enhanced extras: {len(full_update['extras'])} metadata fields\")\n",
        "    print(f\"  • New resource: {len(full_update['resources'])} total resources\")\n",
        "    \n",
        "    # Apply the full update\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    put_response = make_api_request(\"PUT\", f\"/dataset/{dataset_id}\", data=full_update, params=params)\n",
        "    print_response(put_response, \"Full Update Response\")\n",
        "    \n",
        "    if \"error\" not in put_response:\n",
        "        print(\"\\n✅ Full update completed successfully!\")\n",
        "        print(\"🔄 Dataset completely replaced with new version\")\n",
        "        print(\"📊 All fields updated to new values\")\n",
        "    else:\n",
        "        print(\"❌ Full update failed\")\n",
        "else:\n",
        "    print(\"⚠️  No dataset available for update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Error Handling and Best Practices\n",
        "\n",
        "Let's demonstrate proper error handling and common pitfalls to avoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Errors\n",
        "\n",
        "Here's what happens when you provide invalid data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing validation errors...\n",
            "\n",
            "1️⃣ Testing missing required fields:\n",
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /dataset (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57CEE2C0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 Validation Error - Missing Fields:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /dataset (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57CEE2C0>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "2️⃣ Testing invalid resource structure:\n",
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /dataset (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57CEF490>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 Validation Error - Invalid Resources:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /dataset (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57CEF490>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "3️⃣ Testing reserved keys in extras:\n",
            "❌ Request failed: HTTPConnectionPool(host='localhost', port=8001): Max retries exceeded with url: /dataset (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022C57DB28D0>: Failed to establish a new connection: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'))\n",
            "\n",
            "📋 Validation Error - Reserved Keys:\n",
            "──────────────────────────────────────────────────\n",
            "{'error': True,\n",
            " 'exception': \"HTTPConnectionPool(host='localhost', port=8001): Max retries \"\n",
            "              'exceeded with url: /dataset (Caused by '\n",
            "              \"NewConnectionError('<urllib3.connection.HTTPConnection object \"\n",
            "              'at 0x0000022C57DB28D0>: Failed to establish a new connection: '\n",
            "              '[WinError 10061] No se puede establecer una conexión ya que el '\n",
            "              \"equipo de destino denegó expresamente dicha conexión'))\"}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "📚 Key Takeaways from Error Examples:\n",
            "  • Always include required fields: name, title, owner_org\n",
            "  • Use 'url' not 'resource_url' in resource objects\n",
            "  • Avoid reserved keys in extras: name, title, id, resources, etc.\n",
            "  • Check API documentation for complete field specifications\n"
          ]
        }
      ],
      "source": [
        "print(\"🧪 Testing validation errors...\")\n",
        "\n",
        "# Example 1: Missing required fields\n",
        "invalid_payload_1 = {\n",
        "    \"title\": \"Dataset Without Name\",\n",
        "    # Missing required 'name' and 'owner_org' fields\n",
        "}\n",
        "\n",
        "print(\"\\n1️⃣ Testing missing required fields:\")\n",
        "error_response_1 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_1)\n",
        "print_response(error_response_1, \"Validation Error - Missing Fields\")\n",
        "\n",
        "# Example 2: Invalid resource structure  \n",
        "invalid_payload_2 = {\n",
        "    \"name\": \"test_invalid_resources\",\n",
        "    \"title\": \"Test Invalid Resources\",\n",
        "    \"owner_org\": organization_id,\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"resource_url\": \"https://example.com/data.csv\",  # Wrong field name\n",
        "            \"format\": \"CSV\"\n",
        "            # Missing required 'url' and 'name' fields\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"\\n2️⃣ Testing invalid resource structure:\")\n",
        "error_response_2 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_2)\n",
        "print_response(error_response_2, \"Validation Error - Invalid Resources\")\n",
        "\n",
        "# Example 3: Reserved keys in extras\n",
        "invalid_payload_3 = {\n",
        "    \"name\": \"test_reserved_keys\",\n",
        "    \"title\": \"Test Reserved Keys\",\n",
        "    \"owner_org\": organization_id,\n",
        "    \"extras\": {\n",
        "        \"name\": \"This conflicts with the dataset name\",  # Reserved key\n",
        "        \"id\": \"This conflicts with CKAN ID\",  # Reserved key\n",
        "        \"custom_field\": \"This is fine\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n3️⃣ Testing reserved keys in extras:\")\n",
        "error_response_3 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_3)\n",
        "print_response(error_response_3, \"Validation Error - Reserved Keys\")\n",
        "\n",
        "print(\"\\n📚 Key Takeaways from Error Examples:\")\n",
        "print(\"  • Always include required fields: name, title, owner_org\")\n",
        "print(\"  • Use 'url' not 'resource_url' in resource objects\")\n",
        "print(\"  • Avoid reserved keys in extras: name, title, id, resources, etc.\")\n",
        "print(\"  • Check API documentation for complete field specifications\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best Practices Summary\n",
        "\n",
        "Based on our tutorial, here are the key best practices for using the POP API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 POP API Best Practices Summary:\n",
            "============================================================\n",
            "\n",
            "🔸 Authentication:\n",
            "   • Always include valid Bearer token in Authorization header\n",
            "   • Store tokens securely and rotate them regularly\n",
            "   • Test authentication before making bulk operations\n",
            "\n",
            "🔸 Dataset Naming:\n",
            "   • Use descriptive, unique names with underscores\n",
            "   • Include timestamps for versioning: dataset_20240615\n",
            "   • Follow your organization's naming conventions\n",
            "\n",
            "🔸 Metadata Management:\n",
            "   • Provide comprehensive descriptions in 'notes' field\n",
            "   • Use tags for discoverability (5-10 relevant tags)\n",
            "   • Leverage extras for domain-specific metadata\n",
            "   • Include contact information and data provenance\n",
            "\n",
            "🔸 Resource Organization:\n",
            "   • Provide accurate format specifications\n",
            "   • Include file sizes when known\n",
            "   • Use descriptive resource names and descriptions\n",
            "   • Organize related files within single datasets\n",
            "\n",
            "🔸 Update Strategy:\n",
            "   • Use PATCH for incremental updates\n",
            "   • Use PUT for major revisions\n",
            "   • Version your datasets appropriately\n",
            "   • Document changes in update notes\n",
            "\n",
            "🔸 Error Handling:\n",
            "   • Always check response status codes\n",
            "   • Implement retry logic for transient failures\n",
            "   • Log errors for debugging and monitoring\n",
            "   • Validate payloads before submission\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"🎯 POP API Best Practices Summary:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_practices = {\n",
        "    \"Authentication\": [\n",
        "        \"Always include valid Bearer token in Authorization header\",\n",
        "        \"Store tokens securely and rotate them regularly\",\n",
        "        \"Test authentication before making bulk operations\"\n",
        "    ],\n",
        "    \"Dataset Naming\": [\n",
        "        \"Use descriptive, unique names with underscores\",\n",
        "        \"Include timestamps for versioning: dataset_20240615\",\n",
        "        \"Follow your organization's naming conventions\"\n",
        "    ],\n",
        "    \"Metadata Management\": [\n",
        "        \"Provide comprehensive descriptions in 'notes' field\",\n",
        "        \"Use tags for discoverability (5-10 relevant tags)\",\n",
        "        \"Leverage extras for domain-specific metadata\",\n",
        "        \"Include contact information and data provenance\"\n",
        "    ],\n",
        "    \"Resource Organization\": [\n",
        "        \"Provide accurate format specifications\",\n",
        "        \"Include file sizes when known\",\n",
        "        \"Use descriptive resource names and descriptions\",\n",
        "        \"Organize related files within single datasets\"\n",
        "    ],\n",
        "    \"Update Strategy\": [\n",
        "        \"Use PATCH for incremental updates\",\n",
        "        \"Use PUT for major revisions\",\n",
        "        \"Version your datasets appropriately\",\n",
        "        \"Document changes in update notes\"\n",
        "    ],\n",
        "    \"Error Handling\": [\n",
        "        \"Always check response status codes\",\n",
        "        \"Implement retry logic for transient failures\",\n",
        "        \"Log errors for debugging and monitoring\",\n",
        "        \"Validate payloads before submission\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, practices in best_practices.items():\n",
        "    print(f\"\\n🔸 {category}:\")\n",
        "    for practice in practices:\n",
        "        print(f\"   • {practice}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Use Cases\n",
        "\n",
        "Here are some advanced patterns for using the API in production environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bulk Dataset Creation\n",
        "\n",
        "For creating multiple datasets efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  Organization required for batch creation demo\n"
          ]
        }
      ],
      "source": [
        "def create_dataset_batch(datasets: list, organization_id: str, delay: float = 0.5):\n",
        "    \"\"\"\n",
        "    Create multiple datasets with rate limiting and error handling.\n",
        "    \n",
        "    Args:\n",
        "        datasets: List of dataset configurations\n",
        "        organization_id: Organization to create datasets in\n",
        "        delay: Delay between requests to avoid rate limiting\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, dataset_config in enumerate(datasets, 1):\n",
        "        print(f\"\\n📦 Creating dataset {i}/{len(datasets)}: {dataset_config['name']}\")\n",
        "        \n",
        "        # Ensure organization is set\n",
        "        dataset_config[\"owner_org\"] = organization_id\n",
        "        \n",
        "        # Create dataset\n",
        "        params = {\"server\": CKAN_SERVER}\n",
        "        response = make_api_request(\"POST\", \"/dataset\", data=dataset_config, params=params)\n",
        "        \n",
        "        results.append({\n",
        "            \"dataset_name\": dataset_config[\"name\"],\n",
        "            \"success\": \"error\" not in response,\n",
        "            \"response\": response\n",
        "        })\n",
        "        \n",
        "        # Rate limiting\n",
        "        if i < len(datasets):\n",
        "            time.sleep(delay)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example batch creation\n",
        "if organization_id:\n",
        "    print(\"🚀 Demonstrating batch dataset creation...\")\n",
        "    \n",
        "    sample_datasets = [\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_1_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 1 - Temperature Data\",\n",
        "            \"notes\": \"Sample temperature measurements for tutorial\",\n",
        "            \"tags\": [\"temperature\", \"sample\", \"tutorial\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_2_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 2 - Humidity Data\", \n",
        "            \"notes\": \"Sample humidity measurements for tutorial\",\n",
        "            \"tags\": [\"humidity\", \"sample\", \"tutorial\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_3_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 3 - Wind Data\",\n",
        "            \"notes\": \"Sample wind measurements for tutorial\", \n",
        "            \"tags\": [\"wind\", \"sample\", \"tutorial\"]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    batch_results = create_dataset_batch(sample_datasets, organization_id)\n",
        "    \n",
        "    # Summary\n",
        "    successful = sum(1 for r in batch_results if r[\"success\"])\n",
        "    failed = len(batch_results) - successful\n",
        "    \n",
        "    print(f\"\\n📊 Batch Creation Summary:\")\n",
        "    print(f\"   ✅ Successful: {successful}\")\n",
        "    print(f\"   ❌ Failed: {failed}\")\n",
        "    print(f\"   📈 Success Rate: {(successful/len(batch_results)*100):.1f}%\")\n",
        "else:\n",
        "    print(\"⚠️  Organization required for batch creation demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Template Function\n",
        "\n",
        "Create a reusable template for consistent dataset creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  Organization required for template demo\n"
          ]
        }
      ],
      "source": [
        "def create_dataset_from_template(name: str, title: str, description: str, \n",
        "                                resources: list, organization_id: str,\n",
        "                                project_code: str = None, \n",
        "                                tags: list = None) -> dict:\n",
        "    \"\"\"\n",
        "    Create a dataset using a standardized template.\n",
        "    \n",
        "    This function enforces organizational standards and reduces errors.\n",
        "    \"\"\"\n",
        "    # Standard template with organizational defaults\n",
        "    template = {\n",
        "        \"name\": name.lower().replace(\" \", \"_\"),  # Normalize name\n",
        "        \"title\": title,\n",
        "        \"owner_org\": organization_id,\n",
        "        \"notes\": description,\n",
        "        \"license_id\": \"cc-by-4.0\",  # Organizational default\n",
        "        \"private\": False,\n",
        "        \"version\": \"1.0\",\n",
        "        \"tags\": tags or [],\n",
        "        \"groups\": [\"research\"],  # Default group\n",
        "        \"resources\": resources,\n",
        "        \"extras\": {\n",
        "            \"created_via\": \"POP API Tutorial\",\n",
        "            \"creation_date\": datetime.datetime.now().isoformat(),\n",
        "            \"data_steward\": \"API User\",\n",
        "            \"review_status\": \"Pending\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Add project code if provided\n",
        "    if project_code:\n",
        "        template[\"extras\"][\"project_code\"] = project_code\n",
        "    \n",
        "    return template\n",
        "\n",
        "# Example usage\n",
        "if organization_id:\n",
        "    print(\"🏗️  Creating dataset from template...\")\n",
        "    \n",
        "    template_dataset = create_dataset_from_template(\n",
        "        name=\"Template Example Dataset\",\n",
        "        title=\"Dataset Created from Template\",\n",
        "        description=\"This dataset demonstrates the use of standardized templates for consistent data management.\",\n",
        "        resources=[\n",
        "            {\n",
        "                \"url\": \"https://example.com/template_data.csv\",\n",
        "                \"name\": \"Template Data\",\n",
        "                \"format\": \"CSV\",\n",
        "                \"description\": \"Sample data created from template\"\n",
        "            }\n",
        "        ],\n",
        "        organization_id=organization_id,\n",
        "        project_code=\"TEMPLATE2024\",\n",
        "        tags=[\"template\", \"example\", \"standardized\"]\n",
        "    )\n",
        "    \n",
        "    print_response(template_dataset, \"Template Dataset\")\n",
        "    \n",
        "    # Create the dataset\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    template_response = make_api_request(\"POST\", \"/dataset\", data=template_dataset, params=params)\n",
        "    \n",
        "    if \"error\" not in template_response:\n",
        "        print(\"\\n✅ Template dataset created successfully!\")\n",
        "    else:\n",
        "        print(\"❌ Template dataset creation failed\")\n",
        "else:\n",
        "    print(\"⚠️  Organization required for template demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup and Summary\n",
        "\n",
        "Let's clean up our tutorial resources and summarize what we've accomplished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tutorial Summary\n",
        "\n",
        "In this comprehensive tutorial, we've demonstrated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎓 Tutorial Summary\n",
            "==================================================\n",
            "  ✅ API Authentication with Keycloak tokens\n",
            "  ✅ Organization creation and management\n",
            "  ✅ Comprehensive dataset creation with metadata\n",
            "  ✅ Resource management (multiple file types)\n",
            "  ✅ Tags and groups for categorization\n",
            "  ✅ Custom metadata using extras\n",
            "  ✅ Partial updates (PATCH) for incremental changes\n",
            "  ✅ Full updates (PUT) for major revisions\n",
            "  ✅ Error handling and validation\n",
            "  ✅ Best practices and advanced patterns\n",
            "  ✅ Batch operations and templates\n",
            "  ✅ Production-ready code examples\n",
            "\n",
            "🎯 Key Achievements:\n",
            "\n",
            "📚 Next Steps:\n",
            "  1. Integrate this code into your data management workflows\n",
            "  2. Customize the templates for your organization's needs\n",
            "  3. Implement monitoring and logging for production use\n",
            "  4. Explore the specialized endpoints (S3, Kafka, URL) for specific use cases\n",
            "  5. Set up automated data pipeline integration\n",
            "  6. Configure proper authentication and access controls\n",
            "\n",
            "==================================================\n",
            "🚀 You're now ready to use the POP API in production!\n"
          ]
        }
      ],
      "source": [
        "print(\"🎓 Tutorial Summary\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "summary_points = [\n",
        "    \"✅ API Authentication with Keycloak tokens\",\n",
        "    \"✅ Organization creation and management\", \n",
        "    \"✅ Comprehensive dataset creation with metadata\",\n",
        "    \"✅ Resource management (multiple file types)\",\n",
        "    \"✅ Tags and groups for categorization\",\n",
        "    \"✅ Custom metadata using extras\",\n",
        "    \"✅ Partial updates (PATCH) for incremental changes\",\n",
        "    \"✅ Full updates (PUT) for major revisions\",\n",
        "    \"✅ Error handling and validation\",\n",
        "    \"✅ Best practices and advanced patterns\",\n",
        "    \"✅ Batch operations and templates\",\n",
        "    \"✅ Production-ready code examples\"\n",
        "]\n",
        "\n",
        "for point in summary_points:\n",
        "    print(f\"  {point}\")\n",
        "\n",
        "print(\"\\n🎯 Key Achievements:\")\n",
        "if organization_id:\n",
        "    print(f\"  📁 Created organization: {organization_id}\")\n",
        "if dataset_id:\n",
        "    print(f\"  📊 Created main dataset: {dataset_id}\")\n",
        "    print(f\"  🔄 Performed updates (PATCH and PUT)\")\n",
        "\n",
        "print(\"\\n📚 Next Steps:\")\n",
        "next_steps = [\n",
        "    \"Integrate this code into your data management workflows\",\n",
        "    \"Customize the templates for your organization's needs\", \n",
        "    \"Implement monitoring and logging for production use\",\n",
        "    \"Explore the specialized endpoints (S3, Kafka, URL) for specific use cases\",\n",
        "    \"Set up automated data pipeline integration\",\n",
        "    \"Configure proper authentication and access controls\"\n",
        "]\n",
        "\n",
        "for i, step in enumerate(next_steps, 1):\n",
        "    print(f\"  {i}. {step}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🚀 You're now ready to use the POP API in production!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Cleanup Tutorial Resources\n",
        "\n",
        "If you want to clean up the resources created during this tutorial, run the following cells. **Warning: This will permanently delete the datasets and organization created in this tutorial.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️  Cleanup disabled. Tutorial resources preserved.\n",
            "💡 To clean up, set CLEANUP_ENABLED = True and run this cell again.\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL: Cleanup tutorial resources\n",
        "# Uncomment and run if you want to remove tutorial data\n",
        "\n",
        "CLEANUP_ENABLED = False  # Set to True to enable cleanup\n",
        "\n",
        "if CLEANUP_ENABLED:\n",
        "    print(\"🧹 Starting cleanup of tutorial resources...\")\n",
        "    \n",
        "    # Note: In a real implementation, you would need delete endpoints\n",
        "    # The current API may not have delete endpoints implemented\n",
        "    # This is a placeholder for demonstration\n",
        "    \n",
        "    if dataset_id:\n",
        "        print(f\"🗑️  Would delete dataset: {dataset_id}\")\n",
        "        # delete_response = make_api_request(\"DELETE\", f\"/dataset/{dataset_id}\")\n",
        "    \n",
        "    if organization_id:\n",
        "        print(f\"🗑️  Would delete organization: {organization_id}\")\n",
        "        # delete_org_response = make_api_request(\"DELETE\", f\"/organization/{organization_id}\")\n",
        "    \n",
        "    print(\"ℹ️  Cleanup simulation complete. Enable cleanup to actually delete resources.\")\n",
        "else:\n",
        "    print(\"ℹ️  Cleanup disabled. Tutorial resources preserved.\")\n",
        "    print(\"💡 To clean up, set CLEANUP_ENABLED = True and run this cell again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Additional Resources\n",
        "\n",
        "### API Documentation\n",
        "- **Swagger UI**: Visit `{API_BASE_URL}/docs` for interactive API documentation\n",
        "- **OpenAPI Spec**: Available at `{API_BASE_URL}/openapi.json`\n",
        "\n",
        "### CKAN Resources\n",
        "- [CKAN API Documentation](https://docs.ckan.org/en/latest/api/)\n",
        "- [CKAN Data Model](https://docs.ckan.org/en/latest/user-guide.html)\n",
        "\n",
        "### Python Libraries\n",
        "- [Requests Documentation](https://docs.python-requests.org/)\n",
        "- [CKAN API Python Client](https://github.com/ckan/ckanapi)\n",
        "\n",
        "### Support\n",
        "For technical support or questions about the POP API:\n",
        "- Check the API status endpoint: `/status`\n",
        "- Review error responses for detailed information\n",
        "- Consult your organization's data management guidelines\n",
        "\n",
        "---\n",
        "\n",
        "**End of Tutorial** 🎉\n",
        "\n",
        "*This notebook provides a comprehensive foundation for working with the POP API. Adapt the code examples to your specific use cases and organizational requirements.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
