{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POP API Tutorial: General Dataset Management\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sci-ndp/pop/blob/main/docs/general_dataset_api_tutorial.ipynb)\n",
        "[![Open in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sci-ndp/pop/main?filepath=docs/general_dataset_api_tutorial.ipynb)\n",
        "\n",
        "> 🚀 **Run Online Options:**\n",
        "> - **Google Colab**: Dependencies installed automatically in the first cell\n",
        "> - **Binder**: Pre-configured environment, ready to run immediately\n",
        "> - **Local**: Requires `pip install requests jupyter`\n",
        "\n",
        "This notebook demonstrates how to use the POP (Point of Presence) API to manage datasets programmatically. You will learn how to:\n",
        "\n",
        "1. **Authenticate** with the API using Keycloak tokens\n",
        "2. **Create organizations** to organize your datasets\n",
        "3. **Create comprehensive datasets** with metadata, tags, groups, and resources\n",
        "4. **Update and manage** existing datasets\n",
        "5. **Handle errors** and best practices\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.7+\n",
        "- `requests` library\n",
        "- Access to a POP API instance\n",
        "- Valid authentication credentials\n",
        "\n",
        "## API Overview\n",
        "\n",
        "The POP API provides both specialized endpoints (for S3, Kafka, URLs) and general dataset endpoints for flexible data management. This tutorial focuses on the general dataset endpoints which offer maximum flexibility for diverse data types and structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install requests -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n",
        "\n",
        "First, let's import the necessary libraries and configure our API connection parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from typing import Dict, Any, Optional\n",
        "import time\n",
        "\n",
        "# Pretty printing for JSON responses\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuration Variables\n",
        "\n",
        "**Important:** Replace these values with your actual API endpoint and credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Base URL: http://localhost:8003\n",
            "CKAN Server: local\n",
            "Token configured: ✓\n"
          ]
        }
      ],
      "source": [
        "# API Configuration\n",
        "API_BASE_URL = \"http://localhost:8003\"  # Replace with your API URL\n",
        "CKAN_SERVER = \"local\"  # Options: \"local\" or \"pre_ckan\"\n",
        "\n",
        "# Authentication Token\n",
        "# You can obtain this token from your Keycloak instance or use the /token endpoint\n",
        "AUTH_TOKEN = \"your_token_from_keycloak\"  # Replace with your actual token\n",
        "\n",
        "# Request headers with authentication\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "print(f\"API Base URL: {API_BASE_URL}\")\n",
        "print(f\"CKAN Server: {CKAN_SERVER}\")\n",
        "print(f\"Token configured: {'✓' if AUTH_TOKEN != 'your-keycloak-token-here' else '✗ Please set your token'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Functions\n",
        "\n",
        "Let's create some utility functions to make our API interactions cleaner and more robust."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_api_request(method: str, endpoint: str, data: Optional[Dict] = None, \n",
        "                    params: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Make an API request with proper error handling.\n",
        "    \n",
        "    Args:\n",
        "        method: HTTP method (GET, POST, PUT, PATCH, DELETE)\n",
        "        endpoint: API endpoint (e.g., '/dataset', '/organization')\n",
        "        data: Request payload for POST/PUT/PATCH requests\n",
        "        params: Query parameters\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing the response data\n",
        "    \"\"\"\n",
        "    url = f\"{API_BASE_URL}{endpoint}\"\n",
        "    \n",
        "    try:\n",
        "        response = requests.request(\n",
        "            method=method,\n",
        "            url=url,\n",
        "            headers=HEADERS,\n",
        "            json=data,\n",
        "            params=params\n",
        "        )\n",
        "        \n",
        "        print(f\"🔗 {method} {url}\")\n",
        "        print(f\"📊 Status: {response.status_code}\")\n",
        "        \n",
        "        if response.status_code in [200, 201]:\n",
        "            result = response.json()\n",
        "            print(\"✅ Success!\")\n",
        "            return result\n",
        "        else:\n",
        "            print(f\"❌ Error: {response.status_code}\")\n",
        "            try:\n",
        "                error_detail = response.json()\n",
        "                print(f\"Error details: {json.dumps(error_detail, indent=2)}\")\n",
        "            except:\n",
        "                print(f\"Error text: {response.text}\")\n",
        "            return {\"error\": True, \"status_code\": response.status_code, \"detail\": response.text}\n",
        "            \n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request failed: {e}\")\n",
        "        return {\"error\": True, \"exception\": str(e)}\n",
        "\n",
        "def print_response(response: Dict[str, Any], title: str = \"Response\"):\n",
        "    \"\"\"\n",
        "    Pretty print API responses.\n",
        "    \"\"\"\n",
        "    print(f\"\\n📋 {title}:\")\n",
        "    print(\"─\" * 50)\n",
        "    pprint(response)\n",
        "    print(\"─\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Authentication Test\n",
        "\n",
        "Let's verify that our authentication is working by checking the API status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔗 GET http://localhost:8003/status\n",
            "📊 Status: 200\n",
            "✅ Success!\n",
            "\n",
            "📋 API Status:\n",
            "──────────────────────────────────────────────────\n",
            "{'ckan_is_active_global': True,\n",
            " 'ckan_is_active_local': True,\n",
            " 'ckan_local_enabled': True,\n",
            " 'keycloak_is_active': True}\n",
            "──────────────────────────────────────────────────\n",
            "🎉 API is accessible and responding correctly!\n"
          ]
        }
      ],
      "source": [
        "# Test API connectivity and authentication\n",
        "status_response = make_api_request(\"GET\", \"/status\")\n",
        "print_response(status_response, \"API Status\")\n",
        "\n",
        "if \"error\" not in status_response:\n",
        "    print(\"🎉 API is accessible and responding correctly!\")\n",
        "else:\n",
        "    print(\"⚠️  API connectivity issues. Please check your configuration.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Organization Management\n",
        "\n",
        "Before creating datasets, we need to ensure we have an organization to contain them. Organizations in CKAN serve as containers and provide access control for datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Existing Organizations\n",
        "\n",
        "First, let's see what organizations already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔗 GET http://localhost:8003/organization\n",
            "📊 Status: 200\n",
            "✅ Success!\n",
            "\n",
            "📋 Existing Organizations:\n",
            "──────────────────────────────────────────────────\n",
            "['research_tutorial_20250628_230121', 'tests_to_delete', 'services']\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "📈 Found 3 organizations\n",
            "Organizations:\n",
            "  1. research_tutorial_20250628_230121\n",
            "  2. tests_to_delete\n",
            "  3. services\n"
          ]
        }
      ],
      "source": [
        "# List existing organizations\n",
        "params = {\"server\": CKAN_SERVER}\n",
        "organizations = make_api_request(\"GET\", \"/organization\", params=params)\n",
        "print_response(organizations, \"Existing Organizations\")\n",
        "\n",
        "if isinstance(organizations, list):\n",
        "    print(f\"\\n📈 Found {len(organizations)} organizations\")\n",
        "    if organizations:\n",
        "        print(\"Organizations:\")\n",
        "        for i, org in enumerate(organizations[:5], 1):  # Show first 5\n",
        "            print(f\"  {i}. {org}\")\n",
        "        if len(organizations) > 5:\n",
        "            print(f\"  ... and {len(organizations) - 5} more\")\n",
        "else:\n",
        "    print(\"⚠️  Unable to retrieve organizations list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a New Organization\n",
        "\n",
        "Now let's create a new organization for our tutorial. We'll use a timestamped name to avoid conflicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏢 Creating organization with data:\n",
            "\n",
            "📋 Organization Data:\n",
            "──────────────────────────────────────────────────\n",
            "{'description': 'An organization created for the POP API tutorial '\n",
            "                'demonstration. This organization showcases best practices for '\n",
            "                'data management and dataset organization.',\n",
            " 'name': 'research_tutorial_20250630_112306',\n",
            " 'title': 'Research Tutorial Organization - 20250630_112306'}\n",
            "──────────────────────────────────────────────────\n",
            "🔗 POST http://localhost:8003/organization\n",
            "📊 Status: 201\n",
            "✅ Success!\n",
            "\n",
            "📋 Organization Creation Response:\n",
            "──────────────────────────────────────────────────\n",
            "{'id': 'c3840c07-681e-4b41-8307-da67f3b54fcd',\n",
            " 'message': 'Organization created successfully'}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "✅ Organization created successfully!\n",
            "🆔 Organization ID: research_tutorial_20250630_112306\n",
            "📋 Organization Name: research_tutorial_20250630_112306\n"
          ]
        }
      ],
      "source": [
        "# Organization data\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "organization_data = {\n",
        "    \"name\": f\"research_tutorial_{timestamp}\",\n",
        "    \"title\": f\"Research Tutorial Organization - {timestamp}\",\n",
        "    \"description\": \"An organization created for the POP API tutorial demonstration. This organization showcases best practices for data management and dataset organization.\"\n",
        "}\n",
        "\n",
        "print(\"🏢 Creating organization with data:\")\n",
        "print_response(organization_data, \"Organization Data\")\n",
        "\n",
        "# Create the organization\n",
        "params = {\"server\": CKAN_SERVER}\n",
        "org_response = make_api_request(\"POST\", \"/organization\", data=organization_data, params=params)\n",
        "print_response(org_response, \"Organization Creation Response\")\n",
        "\n",
        "if \"error\" not in org_response and \"id\" in org_response:\n",
        "    organization_id = organization_data[\"name\"]  # Use name as ID for dataset creation\n",
        "    print(f\"\\n✅ Organization created successfully!\")\n",
        "    print(f\"🆔 Organization ID: {organization_id}\")\n",
        "    print(f\"📋 Organization Name: {organization_data['name']}\")\n",
        "else:\n",
        "    print(\"❌ Failed to create organization\")\n",
        "    organization_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Dataset Creation\n",
        "\n",
        "Now we'll create a comprehensive dataset using the general dataset endpoint. This dataset will demonstrate all the available features including metadata, tags, groups, and multiple resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Dataset Payload\n",
        "\n",
        "Let's prepare a comprehensive dataset that showcases the full capabilities of the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Prepared comprehensive dataset payload:\n",
            "\n",
            "📋 Dataset Payload:\n",
            "──────────────────────────────────────────────────\n",
            "{'extras': {'access_restrictions': 'None - Open Access',\n",
            "            'backup_location': 'Cloud Storage Tier 1',\n",
            "            'contact_email': 'climate-data@university.edu',\n",
            "            'data_collection_period': '2023-01-01 to 2024-12-31',\n",
            "            'data_format_standard': 'CF-1.8',\n",
            "            'doi': '10.5194/essd-2024-example',\n",
            "            'funding_agency': 'National Science Foundation',\n",
            "            'geographical_coverage': 'North America',\n",
            "            'methodology': 'Automated weather station network',\n",
            "            'principal_investigator': 'Dr. Jane Smith',\n",
            "            'project_code': 'CLIMATE2024',\n",
            "            'publication_status': 'Published',\n",
            "            'quality_level': 'Level 2 - Processed',\n",
            "            'temporal_resolution': 'Daily',\n",
            "            'update_frequency': 'Monthly'},\n",
            " 'groups': ['science', 'climate', 'research'],\n",
            " 'license_id': 'cc-by-4.0',\n",
            " 'name': 'climate_research_dataset_20250630_112306',\n",
            " 'notes': 'This dataset contains comprehensive climate research data including '\n",
            "          'temperature, precipitation, and atmospheric measurements. Created '\n",
            "          'as part of the POP API tutorial to demonstrate best practices for '\n",
            "          'dataset management and metadata organization.',\n",
            " 'owner_org': 'research_tutorial_20250630_112306',\n",
            " 'private': False,\n",
            " 'resources': [{'description': 'Daily temperature readings from 500+ weather '\n",
            "                               'stations across North America. Includes min, '\n",
            "                               'max, and average temperatures with quality '\n",
            "                               'flags.',\n",
            "                'format': 'CSV',\n",
            "                'mimetype': 'text/csv',\n",
            "                'name': 'Daily Temperature Measurements',\n",
            "                'size': 15728640,\n",
            "                'url': 'https://data.climate-research.org/temperature/daily_temp_2024.csv'},\n",
            "               {'description': 'Monthly precipitation totals with statistical '\n",
            "                               'summaries and anomaly calculations relative to '\n",
            "                               '30-year climate normals.',\n",
            "                'format': 'JSON',\n",
            "                'mimetype': 'application/json',\n",
            "                'name': 'Monthly Precipitation Data',\n",
            "                'size': 2097152,\n",
            "                'url': 'https://data.climate-research.org/precipitation/monthly_precip_2024.json'},\n",
            "               {'description': 'Comprehensive atmospheric data including '\n",
            "                               'barometric pressure, humidity, wind speed and '\n",
            "                               'direction, formatted according to '\n",
            "                               'meteorological XML standards.',\n",
            "                'format': 'XML',\n",
            "                'mimetype': 'application/xml',\n",
            "                'name': 'Atmospheric Measurements',\n",
            "                'size': 8388608,\n",
            "                'url': 'https://data.climate-research.org/atmospheric/pressure_humidity.xml'},\n",
            "               {'description': 'High-resolution GeoTIFF raster showing climate '\n",
            "                               'zone classifications based on temperature and '\n",
            "                               'precipitation patterns. Includes metadata for '\n",
            "                               'coordinate reference system.',\n",
            "                'format': 'GeoTIFF',\n",
            "                'mimetype': 'image/tiff',\n",
            "                'name': 'Climate Zone Classification',\n",
            "                'size': 52428800,\n",
            "                'url': 'https://data.climate-research.org/spatial/climate_zones.tif'}],\n",
            " 'tags': ['climate',\n",
            "          'environment',\n",
            "          'timeseries',\n",
            "          'weather',\n",
            "          'research',\n",
            "          'tutorial'],\n",
            " 'title': 'Comprehensive Climate Research Dataset - Tutorial Example',\n",
            " 'version': '1.0'}\n",
            "──────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "# Comprehensive dataset payload\n",
        "dataset_payload = {\n",
        "    # Required fields\n",
        "    \"name\": f\"climate_research_dataset_{timestamp}\",\n",
        "    \"title\": \"Comprehensive Climate Research Dataset - Tutorial Example\",\n",
        "    \"owner_org\": organization_data['name'],  # Use our created organization\n",
        "    \n",
        "    # Descriptive metadata\n",
        "    \"notes\": \"This dataset contains comprehensive climate research data including temperature, precipitation, and atmospheric measurements. Created as part of the POP API tutorial to demonstrate best practices for dataset management and metadata organization.\",\n",
        "    \n",
        "    # Categorization\n",
        "    \"tags\": [\"climate\", \"environment\", \"timeseries\", \"weather\", \"research\", \"tutorial\"],\n",
        "    \"groups\": [\"science\", \"climate\", \"research\"],\n",
        "    \n",
        "    # Administrative metadata\n",
        "    \"license_id\": \"cc-by-4.0\",\n",
        "    \"version\": \"1.0\",\n",
        "    \"private\": False,\n",
        "    \n",
        "    # Custom metadata using extras\n",
        "    \"extras\": {\n",
        "        \"project_code\": \"CLIMATE2024\",\n",
        "        \"funding_agency\": \"National Science Foundation\",\n",
        "        \"principal_investigator\": \"Dr. Jane Smith\",\n",
        "        \"data_collection_period\": \"2023-01-01 to 2024-12-31\",\n",
        "        \"geographical_coverage\": \"North America\",\n",
        "        \"temporal_resolution\": \"Daily\",\n",
        "        \"quality_level\": \"Level 2 - Processed\",\n",
        "        \"contact_email\": \"climate-data@university.edu\",\n",
        "        \"methodology\": \"Automated weather station network\",\n",
        "        \"data_format_standard\": \"CF-1.8\",\n",
        "        \"doi\": \"10.5194/essd-2024-example\",\n",
        "        \"publication_status\": \"Published\",\n",
        "        \"access_restrictions\": \"None - Open Access\",\n",
        "        \"update_frequency\": \"Monthly\",\n",
        "        \"backup_location\": \"Cloud Storage Tier 1\"\n",
        "    },\n",
        "    \n",
        "    # Associated resources\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/temperature/daily_temp_2024.csv\",\n",
        "            \"format\": \"CSV\",\n",
        "            \"name\": \"Daily Temperature Measurements\",\n",
        "            \"description\": \"Daily temperature readings from 500+ weather stations across North America. Includes min, max, and average temperatures with quality flags.\",\n",
        "            \"mimetype\": \"text/csv\",\n",
        "            \"size\": 15728640  # ~15MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/precipitation/monthly_precip_2024.json\",\n",
        "            \"format\": \"JSON\",\n",
        "            \"name\": \"Monthly Precipitation Data\",\n",
        "            \"description\": \"Monthly precipitation totals with statistical summaries and anomaly calculations relative to 30-year climate normals.\",\n",
        "            \"mimetype\": \"application/json\",\n",
        "            \"size\": 2097152  # ~2MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/atmospheric/pressure_humidity.xml\",\n",
        "            \"format\": \"XML\",\n",
        "            \"name\": \"Atmospheric Measurements\",\n",
        "            \"description\": \"Comprehensive atmospheric data including barometric pressure, humidity, wind speed and direction, formatted according to meteorological XML standards.\",\n",
        "            \"mimetype\": \"application/xml\",\n",
        "            \"size\": 8388608  # ~8MB\n",
        "        },\n",
        "        {\n",
        "            \"url\": \"https://data.climate-research.org/spatial/climate_zones.tif\",\n",
        "            \"format\": \"GeoTIFF\",\n",
        "            \"name\": \"Climate Zone Classification\",\n",
        "            \"description\": \"High-resolution GeoTIFF raster showing climate zone classifications based on temperature and precipitation patterns. Includes metadata for coordinate reference system.\",\n",
        "            \"mimetype\": \"image/tiff\",\n",
        "            \"size\": 52428800  # ~50MB\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"📊 Prepared comprehensive dataset payload:\")\n",
        "print_response(dataset_payload, \"Dataset Payload\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the Dataset\n",
        "\n",
        "Now let's create the dataset using our comprehensive payload."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Creating comprehensive dataset...\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 201\n",
            "✅ Success!\n",
            "\n",
            "📋 Dataset Creation Response:\n",
            "──────────────────────────────────────────────────\n",
            "{'id': '03db42a3-44c0-4721-8512-558a5f2f44d0'}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "🎉 Dataset created successfully!\n",
            "🆔 Dataset ID: 03db42a3-44c0-4721-8512-558a5f2f44d0\n",
            "📋 Dataset Name: climate_research_dataset_20250630_112306\n",
            "🏷️  Tags: climate, environment, timeseries, weather, research, tutorial\n",
            "👥 Groups: science, climate, research\n",
            "📁 Resources: 4 files\n",
            "🏢 Organization: research_tutorial_20250630_112306\n"
          ]
        }
      ],
      "source": [
        "if organization_id:\n",
        "    print(\"🚀 Creating comprehensive dataset...\")\n",
        "    \n",
        "    # Create the dataset\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    dataset_response = make_api_request(\"POST\", \"/dataset\", data=dataset_payload, params=params)\n",
        "    print_response(dataset_response, \"Dataset Creation Response\")\n",
        "    \n",
        "    if \"error\" not in dataset_response and \"id\" in dataset_response:\n",
        "        dataset_id = dataset_response[\"id\"]\n",
        "        print(f\"\\n🎉 Dataset created successfully!\")\n",
        "        print(f\"🆔 Dataset ID: {dataset_id}\")\n",
        "        print(f\"📋 Dataset Name: {dataset_payload['name']}\")\n",
        "        print(f\"🏷️  Tags: {', '.join(dataset_payload['tags'])}\")\n",
        "        print(f\"👥 Groups: {', '.join(dataset_payload['groups'])}\")\n",
        "        print(f\"📁 Resources: {len(dataset_payload['resources'])} files\")\n",
        "        print(f\"🏢 Organization: {organization_id}\")\n",
        "    else:\n",
        "        print(\"❌ Failed to create dataset\")\n",
        "        dataset_id = None\n",
        "else:\n",
        "    print(\"⚠️  Cannot create dataset without a valid organization\")\n",
        "    dataset_id = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Updates\n",
        "\n",
        "The API supports both full updates (PUT) and partial updates (PATCH). Let's demonstrate both approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partial Update (PATCH)\n",
        "\n",
        "PATCH updates allow you to modify specific fields without affecting others. This is ideal for incremental changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Performing partial update (PATCH)...\n",
            "📝 Partial update payload:\n",
            "\n",
            "📋 Partial Update Data:\n",
            "──────────────────────────────────────────────────\n",
            "{'extras': {'last_updated': '2025-06-30T11:23:23.477536',\n",
            "            'quality_level': 'Level 3 - Validated',\n",
            "            'revision_notes': 'Enhanced quality control procedures applied',\n",
            "            'update_type': 'Quality enhancement',\n",
            "            'validation_status': 'Completed'},\n",
            " 'notes': 'This dataset contains comprehensive climate research data including '\n",
            "          'temperature, precipitation, and atmospheric measurements. Created '\n",
            "          'as part of the POP API tutorial to demonstrate best practices for '\n",
            "          'dataset management and metadata organization.\\n'\n",
            "          '\\n'\n",
            "          '**Update Log:**\\n'\n",
            "          '- Version 1.1: Added additional quality control measures and '\n",
            "          'enhanced metadata.',\n",
            " 'version': '1.1'}\n",
            "──────────────────────────────────────────────────\n",
            "🔗 PATCH http://localhost:8003/dataset/03db42a3-44c0-4721-8512-558a5f2f44d0\n",
            "📊 Status: 200\n",
            "✅ Success!\n",
            "\n",
            "📋 Partial Update Response:\n",
            "──────────────────────────────────────────────────\n",
            "{'message': 'Dataset updated successfully'}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "✅ Partial update completed successfully!\n",
            "📈 Updated fields: version, notes, extras\n",
            "🔒 Preserved fields: title, tags, groups, resources, etc.\n"
          ]
        }
      ],
      "source": [
        "if dataset_id:\n",
        "    print(\"🔄 Performing partial update (PATCH)...\")\n",
        "    \n",
        "    # Partial update payload - only fields we want to change\n",
        "    partial_update = {\n",
        "        \"version\": \"1.1\",\n",
        "        \"notes\": dataset_payload[\"notes\"] + \"\\n\\n**Update Log:**\\n- Version 1.1: Added additional quality control measures and enhanced metadata.\",\n",
        "        \"extras\": {\n",
        "            \"last_updated\": datetime.datetime.now().isoformat(),\n",
        "            \"update_type\": \"Quality enhancement\",\n",
        "            \"quality_level\": \"Level 3 - Validated\",\n",
        "            \"validation_status\": \"Completed\",\n",
        "            \"revision_notes\": \"Enhanced quality control procedures applied\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(\"📝 Partial update payload:\")\n",
        "    print_response(partial_update, \"Partial Update Data\")\n",
        "    \n",
        "    # Apply the partial update\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    patch_response = make_api_request(\"PATCH\", f\"/dataset/{dataset_id}\", data=partial_update, params=params)\n",
        "    print_response(patch_response, \"Partial Update Response\")\n",
        "    \n",
        "    if \"error\" not in patch_response:\n",
        "        print(\"\\n✅ Partial update completed successfully!\")\n",
        "        print(\"📈 Updated fields: version, notes, extras\")\n",
        "        print(\"🔒 Preserved fields: title, tags, groups, resources, etc.\")\n",
        "    else:\n",
        "        print(\"❌ Partial update failed\")\n",
        "else:\n",
        "    print(\"⚠️  No dataset available for update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Update (PUT)\n",
        "\n",
        "PUT updates replace the entire dataset. This is useful when you want to make comprehensive changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Performing full update (PUT)...\n",
            "📝 Full update includes:\n",
            "  • Updated title: Comprehensive Climate Research Dataset - Updated Edition\n",
            "  • New version: 2.0\n",
            "  • Updated license: cc-by-sa-4.0\n",
            "  • Additional tags: 9 total\n",
            "  • Enhanced extras: 21 metadata fields\n",
            "  • New resource: 5 total resources\n",
            "🔗 PUT http://localhost:8003/dataset/03db42a3-44c0-4721-8512-558a5f2f44d0\n",
            "📊 Status: 200\n",
            "✅ Success!\n",
            "\n",
            "📋 Full Update Response:\n",
            "──────────────────────────────────────────────────\n",
            "{'message': 'Dataset updated successfully'}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "✅ Full update completed successfully!\n",
            "🔄 Dataset completely replaced with new version\n",
            "📊 All fields updated to new values\n"
          ]
        }
      ],
      "source": [
        "if dataset_id:\n",
        "    print(\"🔄 Performing full update (PUT)...\")\n",
        "    \n",
        "    # Full update - modify the original payload\n",
        "    full_update = dataset_payload.copy()\n",
        "    full_update.update({\n",
        "        \"version\": \"2.0\",\n",
        "        \"title\": \"Comprehensive Climate Research Dataset - Updated Edition\",\n",
        "        \"tags\": dataset_payload[\"tags\"] + [\"updated\", \"validated\", \"v2\"],\n",
        "        \"license_id\": \"cc-by-sa-4.0\",  # Changed license\n",
        "    })\n",
        "    \n",
        "    # Update extras with new information\n",
        "    full_update[\"extras\"].update({\n",
        "        \"major_update\": datetime.datetime.now().isoformat(),\n",
        "        \"update_type\": \"Major revision\",\n",
        "        \"quality_level\": \"Level 4 - Research Grade\",\n",
        "        \"peer_review_status\": \"Completed\",\n",
        "        \"publication_doi\": \"10.5194/essd-2024-updated\",\n",
        "        \"citation_count\": \"0\",\n",
        "        \"download_statistics\": \"Available via API\"\n",
        "    })\n",
        "    \n",
        "    # Add a new resource\n",
        "    full_update[\"resources\"].append({\n",
        "        \"url\": \"https://data.climate-research.org/documentation/dataset_methodology_v2.pdf\",\n",
        "        \"format\": \"PDF\",\n",
        "        \"name\": \"Methodology Documentation v2.0\",\n",
        "        \"description\": \"Comprehensive methodology documentation including data collection procedures, quality control measures, and validation protocols for version 2.0.\",\n",
        "        \"mimetype\": \"application/pdf\",\n",
        "        \"size\": 1048576  # ~1MB\n",
        "    })\n",
        "    \n",
        "    print(\"📝 Full update includes:\")\n",
        "    print(f\"  • Updated title: {full_update['title']}\")\n",
        "    print(f\"  • New version: {full_update['version']}\")\n",
        "    print(f\"  • Updated license: {full_update['license_id']}\")\n",
        "    print(f\"  • Additional tags: {len(full_update['tags'])} total\")\n",
        "    print(f\"  • Enhanced extras: {len(full_update['extras'])} metadata fields\")\n",
        "    print(f\"  • New resource: {len(full_update['resources'])} total resources\")\n",
        "    \n",
        "    # Apply the full update\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    put_response = make_api_request(\"PUT\", f\"/dataset/{dataset_id}\", data=full_update, params=params)\n",
        "    print_response(put_response, \"Full Update Response\")\n",
        "    \n",
        "    if \"error\" not in put_response:\n",
        "        print(\"\\n✅ Full update completed successfully!\")\n",
        "        print(\"🔄 Dataset completely replaced with new version\")\n",
        "        print(\"📊 All fields updated to new values\")\n",
        "    else:\n",
        "        print(\"❌ Full update failed\")\n",
        "else:\n",
        "    print(\"⚠️  No dataset available for update\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Error Handling and Best Practices\n",
        "\n",
        "Let's demonstrate proper error handling and common pitfalls to avoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Errors\n",
        "\n",
        "Here's what happens when you provide invalid data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing validation errors...\n",
            "\n",
            "1️⃣ Testing missing required fields:\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 422\n",
            "❌ Error: 422\n",
            "Error details: {\n",
            "  \"detail\": [\n",
            "    {\n",
            "      \"type\": \"missing\",\n",
            "      \"loc\": [\n",
            "        \"body\",\n",
            "        \"name\"\n",
            "      ],\n",
            "      \"msg\": \"Field required\",\n",
            "      \"input\": {\n",
            "        \"title\": \"Dataset Without Name\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"missing\",\n",
            "      \"loc\": [\n",
            "        \"body\",\n",
            "        \"owner_org\"\n",
            "      ],\n",
            "      \"msg\": \"Field required\",\n",
            "      \"input\": {\n",
            "        \"title\": \"Dataset Without Name\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "📋 Validation Error - Missing Fields:\n",
            "──────────────────────────────────────────────────\n",
            "{'detail': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"name\"],\"msg\":\"Field '\n",
            "           'required\",\"input\":{\"title\":\"Dataset Without '\n",
            "           'Name\"}},{\"type\":\"missing\",\"loc\":[\"body\",\"owner_org\"],\"msg\":\"Field '\n",
            "           'required\",\"input\":{\"title\":\"Dataset Without Name\"}}]}',\n",
            " 'error': True,\n",
            " 'status_code': 422}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "2️⃣ Testing invalid resource structure:\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 422\n",
            "❌ Error: 422\n",
            "Error details: {\n",
            "  \"detail\": [\n",
            "    {\n",
            "      \"type\": \"missing\",\n",
            "      \"loc\": [\n",
            "        \"body\",\n",
            "        \"resources\",\n",
            "        0,\n",
            "        \"url\"\n",
            "      ],\n",
            "      \"msg\": \"Field required\",\n",
            "      \"input\": {\n",
            "        \"resource_url\": \"https://example.com/data.csv\",\n",
            "        \"format\": \"CSV\"\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"missing\",\n",
            "      \"loc\": [\n",
            "        \"body\",\n",
            "        \"resources\",\n",
            "        0,\n",
            "        \"name\"\n",
            "      ],\n",
            "      \"msg\": \"Field required\",\n",
            "      \"input\": {\n",
            "        \"resource_url\": \"https://example.com/data.csv\",\n",
            "        \"format\": \"CSV\"\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "📋 Validation Error - Invalid Resources:\n",
            "──────────────────────────────────────────────────\n",
            "{'detail': '{\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"resources\",0,\"url\"],\"msg\":\"Field '\n",
            "           'required\",\"input\":{\"resource_url\":\"https://example.com/data.csv\",\"format\":\"CSV\"}},{\"type\":\"missing\",\"loc\":[\"body\",\"resources\",0,\"name\"],\"msg\":\"Field '\n",
            "           'required\",\"input\":{\"resource_url\":\"https://example.com/data.csv\",\"format\":\"CSV\"}}]}',\n",
            " 'error': True,\n",
            " 'status_code': 422}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "3️⃣ Testing reserved keys in extras:\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 400\n",
            "❌ Error: 400\n",
            "Error details: {\n",
            "  \"detail\": \"Reserved key error: \\\"Extras contain reserved keys: {'id', 'name'}\\\"\"\n",
            "}\n",
            "\n",
            "📋 Validation Error - Reserved Keys:\n",
            "──────────────────────────────────────────────────\n",
            "{'detail': '{\"detail\":\"Reserved key error: \\\\\"Extras contain reserved keys: '\n",
            "           '{\\'id\\', \\'name\\'}\\\\\"\"}',\n",
            " 'error': True,\n",
            " 'status_code': 400}\n",
            "──────────────────────────────────────────────────\n",
            "\n",
            "📚 Key Takeaways from Error Examples:\n",
            "  • Always include required fields: name, title, owner_org\n",
            "  • Use 'url' not 'resource_url' in resource objects\n",
            "  • Avoid reserved keys in extras: name, title, id, resources, etc.\n",
            "  • Check API documentation for complete field specifications\n"
          ]
        }
      ],
      "source": [
        "print(\"🧪 Testing validation errors...\")\n",
        "\n",
        "# Example 1: Missing required fields\n",
        "invalid_payload_1 = {\n",
        "    \"title\": \"Dataset Without Name\",\n",
        "    # Missing required 'name' and 'owner_org' fields\n",
        "}\n",
        "\n",
        "print(\"\\n1️⃣ Testing missing required fields:\")\n",
        "error_response_1 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_1)\n",
        "print_response(error_response_1, \"Validation Error - Missing Fields\")\n",
        "\n",
        "# Example 2: Invalid resource structure  \n",
        "invalid_payload_2 = {\n",
        "    \"name\": \"test_invalid_resources\",\n",
        "    \"title\": \"Test Invalid Resources\",\n",
        "    \"owner_org\": organization_id,\n",
        "    \"resources\": [\n",
        "        {\n",
        "            \"resource_url\": \"https://example.com/data.csv\",  # Wrong field name\n",
        "            \"format\": \"CSV\"\n",
        "            # Missing required 'url' and 'name' fields\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"\\n2️⃣ Testing invalid resource structure:\")\n",
        "error_response_2 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_2)\n",
        "print_response(error_response_2, \"Validation Error - Invalid Resources\")\n",
        "\n",
        "# Example 3: Reserved keys in extras\n",
        "invalid_payload_3 = {\n",
        "    \"name\": \"test_reserved_keys\",\n",
        "    \"title\": \"Test Reserved Keys\",\n",
        "    \"owner_org\": organization_id,\n",
        "    \"extras\": {\n",
        "        \"name\": \"This conflicts with the dataset name\",  # Reserved key\n",
        "        \"id\": \"This conflicts with CKAN ID\",  # Reserved key\n",
        "        \"custom_field\": \"This is fine\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n3️⃣ Testing reserved keys in extras:\")\n",
        "error_response_3 = make_api_request(\"POST\", \"/dataset\", data=invalid_payload_3)\n",
        "print_response(error_response_3, \"Validation Error - Reserved Keys\")\n",
        "\n",
        "print(\"\\n📚 Key Takeaways from Error Examples:\")\n",
        "print(\"  • Always include required fields: name, title, owner_org\")\n",
        "print(\"  • Use 'url' not 'resource_url' in resource objects\")\n",
        "print(\"  • Avoid reserved keys in extras: name, title, id, resources, etc.\")\n",
        "print(\"  • Check API documentation for complete field specifications\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Use Cases\n",
        "\n",
        "Here are some advanced patterns for using the API in production environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bulk Dataset Creation\n",
        "\n",
        "For creating multiple datasets efficiently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Demonstrating batch dataset creation...\n",
            "\n",
            "📦 Creating dataset 1/3: sample_dataset_1_20250630_112306\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 201\n",
            "✅ Success!\n",
            "\n",
            "📦 Creating dataset 2/3: sample_dataset_2_20250630_112306\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 201\n",
            "✅ Success!\n",
            "\n",
            "📦 Creating dataset 3/3: sample_dataset_3_20250630_112306\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 201\n",
            "✅ Success!\n",
            "\n",
            "📊 Batch Creation Summary:\n",
            "   ✅ Successful: 3\n",
            "   ❌ Failed: 0\n",
            "   📈 Success Rate: 100.0%\n"
          ]
        }
      ],
      "source": [
        "def create_dataset_batch(datasets: list, organization_id: str, delay: float = 0.5):\n",
        "    \"\"\"\n",
        "    Create multiple datasets with rate limiting and error handling.\n",
        "    \n",
        "    Args:\n",
        "        datasets: List of dataset configurations\n",
        "        organization_id: Organization to create datasets in\n",
        "        delay: Delay between requests to avoid rate limiting\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, dataset_config in enumerate(datasets, 1):\n",
        "        print(f\"\\n📦 Creating dataset {i}/{len(datasets)}: {dataset_config['name']}\")\n",
        "        \n",
        "        # Ensure organization is set\n",
        "        dataset_config[\"owner_org\"] = organization_id\n",
        "        \n",
        "        # Create dataset\n",
        "        params = {\"server\": CKAN_SERVER}\n",
        "        response = make_api_request(\"POST\", \"/dataset\", data=dataset_config, params=params)\n",
        "        \n",
        "        results.append({\n",
        "            \"dataset_name\": dataset_config[\"name\"],\n",
        "            \"success\": \"error\" not in response,\n",
        "            \"response\": response\n",
        "        })\n",
        "        \n",
        "        # Rate limiting\n",
        "        if i < len(datasets):\n",
        "            time.sleep(delay)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example batch creation\n",
        "if organization_id:\n",
        "    print(\"🚀 Demonstrating batch dataset creation...\")\n",
        "    \n",
        "    sample_datasets = [\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_1_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 1 - Temperature Data\",\n",
        "            \"notes\": \"Sample temperature measurements for tutorial\",\n",
        "            \"tags\": [\"temperature\", \"sample\", \"tutorial\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_2_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 2 - Humidity Data\", \n",
        "            \"notes\": \"Sample humidity measurements for tutorial\",\n",
        "            \"tags\": [\"humidity\", \"sample\", \"tutorial\"]\n",
        "        },\n",
        "        {\n",
        "            \"name\": f\"sample_dataset_3_{timestamp}\",\n",
        "            \"title\": \"Sample Dataset 3 - Wind Data\",\n",
        "            \"notes\": \"Sample wind measurements for tutorial\", \n",
        "            \"tags\": [\"wind\", \"sample\", \"tutorial\"]\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    batch_results = create_dataset_batch(sample_datasets, organization_id)\n",
        "    \n",
        "    # Summary\n",
        "    successful = sum(1 for r in batch_results if r[\"success\"])\n",
        "    failed = len(batch_results) - successful\n",
        "    \n",
        "    print(f\"\\n📊 Batch Creation Summary:\")\n",
        "    print(f\"   ✅ Successful: {successful}\")\n",
        "    print(f\"   ❌ Failed: {failed}\")\n",
        "    print(f\"   📈 Success Rate: {(successful/len(batch_results)*100):.1f}%\")\n",
        "else:\n",
        "    print(\"⚠️  Organization required for batch creation demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Template Function\n",
        "\n",
        "Create a reusable template for consistent dataset creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️  Creating dataset from template...\n",
            "\n",
            "📋 Template Dataset:\n",
            "──────────────────────────────────────────────────\n",
            "{'extras': {'created_via': 'POP API Tutorial',\n",
            "            'creation_date': '2025-06-30T11:24:50.829230',\n",
            "            'data_steward': 'API User',\n",
            "            'project_code': 'TEMPLATE2024',\n",
            "            'review_status': 'Pending'},\n",
            " 'groups': ['research'],\n",
            " 'license_id': 'cc-by-4.0',\n",
            " 'name': 'template_example_dataset',\n",
            " 'notes': 'This dataset demonstrates the use of standardized templates for '\n",
            "          'consistent data management.',\n",
            " 'owner_org': 'research_tutorial_20250630_112306',\n",
            " 'private': False,\n",
            " 'resources': [{'description': 'Sample data created from template',\n",
            "                'format': 'CSV',\n",
            "                'name': 'Template Data',\n",
            "                'url': 'https://example.com/template_data.csv'}],\n",
            " 'tags': ['template', 'example', 'standardized'],\n",
            " 'title': 'Dataset Created from Template',\n",
            " 'version': '1.0'}\n",
            "──────────────────────────────────────────────────\n",
            "🔗 POST http://localhost:8003/dataset\n",
            "📊 Status: 409\n",
            "❌ Error: 409\n",
            "Error details: {\n",
            "  \"detail\": {\n",
            "    \"error\": \"Duplicate Dataset\",\n",
            "    \"detail\": \"A dataset with the given name already exists.\"\n",
            "  }\n",
            "}\n",
            "❌ Template dataset creation failed\n"
          ]
        }
      ],
      "source": [
        "def create_dataset_from_template(name: str, title: str, description: str, \n",
        "                                resources: list, organization_id: str,\n",
        "                                project_code: str = None, \n",
        "                                tags: list = None) -> dict:\n",
        "    \"\"\"\n",
        "    Create a dataset using a standardized template.\n",
        "    \n",
        "    This function enforces organizational standards and reduces errors.\n",
        "    \"\"\"\n",
        "    # Standard template with organizational defaults\n",
        "    template = {\n",
        "        \"name\": name.lower().replace(\" \", \"_\"),  # Normalize name\n",
        "        \"title\": title,\n",
        "        \"owner_org\": organization_id,\n",
        "        \"notes\": description,\n",
        "        \"license_id\": \"cc-by-4.0\",  # Organizational default\n",
        "        \"private\": False,\n",
        "        \"version\": \"1.0\",\n",
        "        \"tags\": tags or [],\n",
        "        \"groups\": [\"research\"],  # Default group\n",
        "        \"resources\": resources,\n",
        "        \"extras\": {\n",
        "            \"created_via\": \"POP API Tutorial\",\n",
        "            \"creation_date\": datetime.datetime.now().isoformat(),\n",
        "            \"data_steward\": \"API User\",\n",
        "            \"review_status\": \"Pending\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Add project code if provided\n",
        "    if project_code:\n",
        "        template[\"extras\"][\"project_code\"] = project_code\n",
        "    \n",
        "    return template\n",
        "\n",
        "# Example usage\n",
        "if organization_id:\n",
        "    print(\"🏗️  Creating dataset from template...\")\n",
        "    \n",
        "    template_dataset = create_dataset_from_template(\n",
        "        name=\"Template Example Dataset\",\n",
        "        title=\"Dataset Created from Template\",\n",
        "        description=\"This dataset demonstrates the use of standardized templates for consistent data management.\",\n",
        "        resources=[\n",
        "            {\n",
        "                \"url\": \"https://example.com/template_data.csv\",\n",
        "                \"name\": \"Template Data\",\n",
        "                \"format\": \"CSV\",\n",
        "                \"description\": \"Sample data created from template\"\n",
        "            }\n",
        "        ],\n",
        "        organization_id=organization_id,\n",
        "        project_code=\"TEMPLATE2024\",\n",
        "        tags=[\"template\", \"example\", \"standardized\"]\n",
        "    )\n",
        "    \n",
        "    print_response(template_dataset, \"Template Dataset\")\n",
        "    \n",
        "    # Create the dataset\n",
        "    params = {\"server\": CKAN_SERVER}\n",
        "    template_response = make_api_request(\"POST\", \"/dataset\", data=template_dataset, params=params)\n",
        "    \n",
        "    if \"error\" not in template_response:\n",
        "        print(\"\\n✅ Template dataset created successfully!\")\n",
        "    else:\n",
        "        print(\"❌ Template dataset creation failed\")\n",
        "else:\n",
        "    print(\"⚠️  Organization required for template demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Cleanup and Summary\n",
        "\n",
        "Let's clean up our tutorial resources and summarize what we've accomplished."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Cleanup Tutorial Resources\n",
        "\n",
        "If you want to clean up the resources created during this tutorial, run the following cells. **Warning: This will permanently delete the datasets and organization created in this tutorial.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️  Cleanup disabled. Tutorial resources preserved.\n",
            "💡 To clean up, set CLEANUP_ENABLED = True and run this cell again.\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL: Cleanup tutorial resources\n",
        "# Uncomment and run if you want to remove tutorial data\n",
        "\n",
        "CLEANUP_ENABLED = False  # Set to True to enable cleanup\n",
        "\n",
        "if CLEANUP_ENABLED:\n",
        "    print(\"🧹 Starting cleanup of tutorial resources...\")\n",
        "    \n",
        "    # Note: In a real implementation, you would need delete endpoints\n",
        "    # The current API may not have delete endpoints implemented\n",
        "    # This is a placeholder for demonstration\n",
        "    \n",
        "    if dataset_id:\n",
        "        print(f\"🗑️  Would delete dataset: {dataset_id}\")\n",
        "        # delete_response = make_api_request(\"DELETE\", f\"/dataset/{dataset_id}\")\n",
        "    \n",
        "    if organization_id:\n",
        "        print(f\"🗑️  Would delete organization: {organization_id}\")\n",
        "        # delete_org_response = make_api_request(\"DELETE\", f\"/organization/{organization_id}\")\n",
        "    \n",
        "    print(\"ℹ️  Cleanup simulation complete. Enable cleanup to actually delete resources.\")\n",
        "else:\n",
        "    print(\"ℹ️  Cleanup disabled. Tutorial resources preserved.\")\n",
        "    print(\"💡 To clean up, set CLEANUP_ENABLED = True and run this cell again.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
